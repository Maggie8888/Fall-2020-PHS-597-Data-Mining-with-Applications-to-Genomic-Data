# -*- coding: utf-8 -*-
"""HW4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qNPHDEIv5h6Txv7EJ52lZ28Bfj-LITb1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from matplotlib import style
style.use('fivethirtyeight')
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
np.random.seed(1234567)
# simulate data 

X1=np.random.multivariate_normal([0.3,0.3],[[1,0],
        [0,0.01]],100)
X2=np.random.multivariate_normal([5,5],[[1.5,0],
        [0,0.1]],100)
X3=np.random.multivariate_normal([3,3],[[2,0],
        [0,0.01]],100)
y=np.zeros((300,1))
y[0:99,0]=1
y[100:199,0]=2
y[200:299,0]=3

x=np.column_stack((y,np.concatenate((X1,X2,X3), axis=0)))
df=pd.DataFrame(x)

X = df.iloc[:,1:].copy()
target = df.iloc[:,0].copy()


# Standardize the data
for col in X.columns:
    X[col] = StandardScaler().fit_transform(X[col].values.reshape(-1,1))



# Compute the mean vector mu and the mean vector per class mu_k
mu = np.mean(X,axis=0).values.reshape(2,1) # Mean vector mu 
mu_k = []

for i,orchid in enumerate(np.unique(df.iloc[:,0])):
    mu_k.append(np.mean(X.where(df.iloc[:,0]==orchid),axis=0))
mu_k = np.array(mu_k).T


# Compute the Scatter within and Scatter between matrices
data_SW = []
Nc = []
for i,orchid in enumerate(np.unique(df.iloc[:,0])):
    a = np.array(X.where(df.iloc[:,0]==orchid).dropna().values-mu_k[:,i].reshape(1,2))
    data_SW.append(np.dot(a.T,a))
    Nc.append(np.sum(df.iloc[:,0]==orchid))
SW = np.sum(data_SW,axis=0)

SB = np.dot(Nc*np.array(mu_k-mu),np.array(mu_k-mu).T)
   
# Compute the Eigenvalues and Eigenvectors of SW^-1 SB
eigval, eigvec = np.linalg.eig(np.dot(np.linalg.inv(SW),SB))


    
# Select the two largest eigenvalues 
eigen_pairs = [[np.abs(eigval[i]),eigvec[:,i]] for i in range(len(eigval))]
eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0],reverse=True)
w = np.hstack((eigen_pairs[0][1][:,np.newaxis].real,eigen_pairs[1][1][:,np.newaxis].real)) # Select two largest


#  Transform the data with Y=X*w
Y = X.dot(w)

# Plot the data
fig = plt.figure(figsize=(10,10))
ax0 = fig.add_subplot(111)
ax0.set_xlim(-3,3)
ax0.set_ylim(-4,3)

for l,c,m in zip(np.unique(y),['r','g','b'],['s','x','o']):
    ax0.scatter(Y[0],
                Y[1],
               c=c, marker=m, label=l,edgecolors='black')
ax0.legend(loc='upper right')


# Plot the voroni spaces
means = []

for m,target in zip(['s','x','o'],np.unique(y)):
    means.append(np.mean(Y[y==target],axis=0))
    ax0.scatter(np.mean(Y[y==target],axis=0)[0],np.mean(Y[y==target],axis=0)[1],marker=m,c='black',s=100)
   
mesh_x, mesh_y = np.meshgrid(np.linspace(-3,3),np.linspace(-4,3)) 
mesh = []


for i in range(len(mesh_x)):
    for j in range(len(mesh_x[0])):
        date = [mesh_x[i][j],mesh_y[i][j]]
        mesh.append((mesh_x[i][j],mesh_y[i][j]))


NN = KNeighborsClassifier(n_neighbors=1)
NN.fit(means,['r','g',"b"])        
predictions = NN.predict(np.array(mesh))

ax0.scatter(np.array(mesh)[:,0],np.array(mesh)[:,1],color=predictions,alpha=0.3)


plt.show()

## use package to do LDA
x=np.column_stack((y,np.concatenate((X1,X2,X3), axis=0)))
df=pd.DataFrame(x)

X = df.iloc[:,1:].copy()
y = df.iloc[:,0].copy()
clf= LinearDiscriminantAnalysis(n_components=2)
X_r= clf.fit(X, y).transform(X)
colors = ['r', 'g', 'b']
target_names =df.iloc[:,0].astype('category')
lw = 2
plt.figure()
for color, i, target_name in zip(colors, [1, 2, 3], target_names):
    plt.scatter(X_r[y==i,0], X_r[y==i,1], alpha=.8, color=color,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('LDA')

